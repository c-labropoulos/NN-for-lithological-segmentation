{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-labropoulos/NN-for-lithological-segmentation/blob/main/UNETTEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4C83ubARXLg",
        "outputId": "7c37fce4-3d0b-458a-a201-74e90f06539b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibfhdxCUZuk9",
        "outputId": "eb06e571-a226-43ef-e8e3-646a34f34ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.3)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2022.12.7)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix as sk_confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STK0dobUaAxD",
        "outputId": "07bdb978-55cf-4db5-dde2-e1142aa5f623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import the PyTorch library\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "# If a GPU with CUDA support is available, the code will use it for computation.\n",
        "# Otherwise, it will use the CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Print the device being used (either \"cuda\" for GPU or \"cpu\" for CPU)\n",
        "# This will help users to verify if their GPU and CUDA installations are recognized\n",
        "# and if the code is running on the GPU as expected.\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPPQU37BC_N"
      },
      "source": [
        "In the above cell :\n",
        "\n",
        "1.\tWe import the torch library, which is the main PyTorch library for tensor computations and neural network building.\n",
        "2.\tWe check if a GPU with CUDA support is available on the system using torch.cuda.is_available(). If a GPU is available, the code will use it for computation; otherwise, it will use the CPU.\n",
        "3.\tWe create a device variable that holds the device type that will be used for computation (either \"cuda\" for GPU or \"cpu\" for CPU).\n",
        "4.\tWe print the device type being used to help users verify if their GPU and CUDA installations are recognized and if the code is running on the GPU as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9cSejgPRD7R"
      },
      "outputs": [],
      "source": [
        "def read_raster_image(file_path):\n",
        "    \"\"\"\n",
        "    Read a raster image from the provided file path and return the first band's data as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path of the raster image file to be read.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array containing the data of the first band of the raster image.\n",
        "    \"\"\"\n",
        "    # Use the rasterio library to open the raster image file\n",
        "    with rasterio.open(file_path) as src:\n",
        "        # Read the first band of the image using the `read` method\n",
        "        image_data = src.read(1)\n",
        "\n",
        "    # Return the data of the first band as a numpy array\n",
        "    return image_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsNgvh3UBk-_"
      },
      "source": [
        "In this code snippet:\n",
        "1.\tWe define a read_raster_image function that takes a file path as an argument.\n",
        "2.\tUsing the rasterio.open context manager, we open the raster image file at the provided file path.\n",
        "3.\tWe read the first band of the image using the read method of the rasterio src object and store the data in the image_data variable.\n",
        "4.\tAfter the context manager closes the file, we return the data of the first band as a numpy array.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OqKGTtdZs67"
      },
      "outputs": [],
      "source": [
        "def get_file_paths(folder_path, file_extension=\".tif\"):\n",
        "    \"\"\"\n",
        "    Traverse a given folder and its subfolders to find files with the specified file extension,\n",
        "    and return a list of their file paths.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path of the folder to search for files.\n",
        "        file_extension (str, optional): The file extension to search for. Defaults to \".tif\".\n",
        "\n",
        "    Returns:\n",
        "        list: A list of file paths for the files with the specified extension found in the folder and its subfolders.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to store the file paths\n",
        "    file_paths = []\n",
        "\n",
        "    # Use os.walk to traverse the folder and its subfolders\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        # Iterate through the files in the current folder\n",
        "        for file in files:\n",
        "            # Check if the file has the specified extension\n",
        "            if file.endswith(file_extension):\n",
        "                # Append the file path to the list\n",
        "                file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    # Return the list of file paths\n",
        "    return file_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBgdxU8zCc4P"
      },
      "source": [
        "In this code snippet:\n",
        "1.\tWe define a get_file_paths function that takes a folder path and an optional file extension as arguments.\n",
        "2.\tWe initialize an empty list file_paths to store the paths of the files we find.\n",
        "3.\tWe use the os.walk function to traverse the folder and its subfolders, iterating through the files in each folder.\n",
        "4.\tFor each file, we check if it has the specified file extension. If it does, we append its path to the file_paths list.\n",
        "5.\tAfter traversing the entire folder structure, we return the list of file paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8adzu8rVMtI_"
      },
      "outputs": [],
      "source": [
        "def read_class_mapping(file_path):\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    \n",
        "    # Iterate through the DataFrame rows and create a dictionary with pixel values as keys and class names as values\n",
        "    class_mapping = {row['pixel_value']: row['class_name'] for _, row in df.iterrows()}\n",
        "    \n",
        "    return class_mapping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function reads the class mapping from a CSV file and returns a dictionary with pixel values as keys and class names as values. "
      ],
      "metadata": {
        "id": "z75-DAExuKDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzJ4QI-_Mrkf"
      },
      "outputs": [],
      "source": [
        "def iou_metric(y_true, y_pred, class_mapping):\n",
        "    # Initialize an empty list to store IoU scores\n",
        "    iou_scores = []\n",
        "    \n",
        "    # Iterate over the class pixel values in the class mapping\n",
        "    for class_pixel_value in class_mapping:\n",
        "        # Skip the current iteration if the class name is 'nan'\n",
        "        if class_mapping[class_pixel_value] == 'nan':\n",
        "            continue\n",
        "\n",
        "        # Create binary masks for the current class in the true and predicted segmentations\n",
        "        y_true_class = y_true == class_pixel_value\n",
        "        y_pred_class = y_pred == class_pixel_value\n",
        "\n",
        "        # Compute the intersection and union of the true and predicted masks\n",
        "        intersection = np.logical_and(y_true_class, y_pred_class)\n",
        "        union = np.logical_or(y_true_class, y_pred_class)\n",
        "\n",
        "        # Skip the current iteration if the union is empty (i.e., no pixels belong to the current class in both true and predicted masks)\n",
        "        if np.sum(union) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate the IoU score for the current class and append it to the list of IoU scores\n",
        "        iou_score = np.sum(intersection) / np.sum(union)\n",
        "        iou_scores.append(iou_score)\n",
        "\n",
        "    # Return the mean IoU score across all classes\n",
        "    return np.mean(iou_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function calculates the mean Intersection over Union (IoU) metric for each class in the segmentation results. "
      ],
      "metadata": {
        "id": "Tg8uCuoiv5FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_accuracy(y_true, y_pred):\n",
        "    # Count the number of pixels where the true and predicted values are equal\n",
        "    correct_pixels = np.sum(y_true == y_pred)\n",
        "    # Get the total number of pixels in the true mask\n",
        "    total_pixels = y_true.size\n",
        "    # Calculate the pixel accuracy as the ratio of correct pixels to total pixels\n",
        "    return correct_pixels / total_pixels\n",
        "\n"
      ],
      "metadata": {
        "id": "xw3wwNrdinCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function calculates the pixel accuracy, which is the ratio of correctly classified pixels to the total number of pixels. "
      ],
      "metadata": {
        "id": "is1ZKskIwZLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(y_true, y_pred, num_classes):\n",
        "    # Flatten the true and predicted masks into 1D arrays\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    \n",
        "    # Compute the weighted precision score for the given labels\n",
        "    precision = precision_score(y_true_flat, y_pred_flat, labels=np.arange(num_classes), average='weighted')\n",
        "    # Compute the weighted recall score for the given labels\n",
        "    recall = recall_score(y_true_flat, y_pred_flat, labels=np.arange(num_classes), average='weighted')\n",
        "    # Compute the weighted F1-score for the given labels\n",
        "    f1 = f1_score(y_true_flat, y_pred_flat, labels=np.arange(num_classes), average='weighted')\n",
        "    \n",
        "    # Return the computed precision, recall, and F1-score\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_mjbw8piqKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function computes the precision, recall, and F1-score for the given true and predicted segmentation masks."
      ],
      "metadata": {
        "id": "-yqnidAMx8d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
        "\n",
        "def compute_confusion_matrix(y_true, y_pred, num_classes):\n",
        "    # Flatten the true and predicted masks into 1D arrays\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    \n",
        "    # Compute the confusion matrix for the given labels\n",
        "    cm = sk_confusion_matrix(y_true_flat, y_pred_flat, labels=np.arange(num_classes))\n",
        "    \n",
        "    # Return the computed confusion matrix\n",
        "    return cm"
      ],
      "metadata": {
        "id": "j3aoKDR4ircz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5rLLvVNWYB"
      },
      "source": [
        "This function computes the confusion matrix for the given true and predicted segmentation masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9K5NU0-Tk_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RasterDataset(Dataset):\n",
        "    def __init__(self, root_dir, patch_size, stride, train_ratio, is_train=True):\n",
        "        \"\"\"\n",
        "        Initialize the RasterDataset class.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): The root directory containing the raster images.\n",
        "            patch_size (int): The size of the patches to be extracted from the images.\n",
        "            stride (int): The step size used when extracting patches from the images.\n",
        "            train_ratio (float): The ratio of images to be used for training.\n",
        "            is_train (bool, optional): A flag indicating whether the dataset is for training or testing. Defaults to True.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "        self.train_ratio = train_ratio\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.image_paths = self.get_image_paths(root_dir)\n",
        "\n",
        "        num_train = int(len(self.image_paths) * train_ratio)\n",
        "        if is_train:\n",
        "            self.image_paths = self.image_paths[:num_train]\n",
        "        else:\n",
        "            self.image_paths = self.image_paths[num_train:]\n",
        "\n",
        "        self.image_patches = self.create_patches()\n",
        "\n",
        "    def get_image_paths(self, root_dir):\n",
        "        \"\"\"\n",
        "        Get the sorted file paths for all .tif images in the specified directory.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): The directory containing the raster images.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of sorted file paths for the .tif images in the directory.\n",
        "        \"\"\"\n",
        "        image_folder = os.path.join(root_dir)\n",
        "        image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.tif')])\n",
        "        return image_paths\n",
        "\n",
        "    def create_patches(self):\n",
        "        \"\"\"\n",
        "        Extract patches from the images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of image patches.\n",
        "        \"\"\"\n",
        "        image_patches = []\n",
        "\n",
        "        for image_path in self.image_paths:\n",
        "            image = Image.open(image_path)\n",
        "            width, height = image.size\n",
        "\n",
        "            for y in range(0, height - self.patch_size + 1, self.stride):\n",
        "                for x in range(0, width - self.patch_size + 1, self.stride):\n",
        "                    image_patch = image.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
        "                    image_patches.append(image_patch)\n",
        "\n",
        "        return image_patches\n",
        "\n",
        "    def preprocess(self, image):\n",
        "        \"\"\"\n",
        "        Preprocess an image patch, converting it to a PyTorch tensor and normalizing it.\n",
        "\n",
        "        Args:\n",
        "            image (PIL.Image): The image patch to be preprocessed.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The preprocessed image patch as a PyTorch tensor.\n",
        "        \"\"\"\n",
        "        # Convert the PIL Image to a numpy array\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Normalize the image to the range [0, 1]\n",
        "        image = image / 255.0\n",
        "\n",
        "        # Convert the numpy array to a PyTorch tensor and add a channel dimension\n",
        "        image = torch.from_numpy(image).float().unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the number of image patches in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of image patches.\n",
        "        \"\"\"\n",
        "        return len(self.image_patches)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Get an image patch and its corresponding mask from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): The index of the image patch in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the image patch and its corresponding mask as PyTorch tensors.\n",
        "        \"\"\"\n",
        "        image_patch = self.image_patches[index]\n",
        "        image_patch = self.preprocess(image_patch)\n",
        "        mask = (image_patch != 0).long() # Convert the mask tensor to Long\n",
        "        mask = mask.squeeze(0) # Remove the channel dimension\n",
        "        return image_patch, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8SiDPA5bAjC"
      },
      "outputs": [],
      "source": [
        "# Create dataset and data loader\n",
        "folder_path = \"/content/drive/MyDrive/raster_to_be_used\"\n",
        "\n",
        "# Set the parameters for creating the dataset\n",
        "patch_size = 128  # Size of the patches extracted from the images\n",
        "stride = 32  # Stride used when extracting patches from the images\n",
        "train_ratio = 0.8  # Ratio of the dataset to be used for training\n",
        "\n",
        "# Create the training dataset using the RasterDataset class\n",
        "train_dataset = RasterDataset(folder_path, patch_size, stride, train_ratio, is_train=True)\n",
        "# Create the DataLoader for the training dataset, with a batch size of 4, shuffling the data, and using 2 workers for parallel processing\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Create the testing dataset using the RasterDataset class\n",
        "test_dataset = RasterDataset(folder_path, patch_size, stride, train_ratio, is_train=False)\n",
        "# Create the DataLoader for the testing dataset, with a batch size of 4, without shuffling the data, and using 2 workers for parallel processing\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vl21FWF4ZA"
      },
      "source": [
        "In the above code snippet, we create the datasets and data loaders for both training and testing. The parameters for patch size, stride, and train ratio are set. The DataLoader objects are created with specific batch sizes, shuffling options, and numbers of parallel processing workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waMf4K-vX71m"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Expanding path\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = self.conv_block(1024, 512)\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = self.conv_block(512, 256)\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = self.conv_block(256, 128)\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        upconv4 = self.upconv4(bottleneck)\n",
        "        dec4 = self.dec4(torch.cat((enc4, upconv4), dim=1))\n",
        "        upconv3 = self.upconv3(dec4)\n",
        "        dec3 = self.dec3(torch.cat((enc3, upconv3), dim=1))\n",
        "        upconv2 = self.upconv2(dec3)\n",
        "        dec2 = self.dec2(torch.cat((enc2, upconv2), dim=1))\n",
        "        upconv1 = self.upconv1(dec2)\n",
        "        dec1 = self.dec1(torch.cat((enc1, upconv1), dim=1))\n",
        "\n",
        "        return self.final_conv(dec1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctbld2QuIUab"
      },
      "source": [
        "In the code snippet above, a UNet class is defined, which inherits from PyTorch's nn.Module. The U-Net architecture consists of an encoder (contracting path), a bottleneck, and a decoder (expanding path). The U-Net is designed for semantic segmentation tasks.\n",
        "The __init__ method of the UNet class defines the layers in the contracting path, the bottleneck, and the expanding path.\n",
        "The forward method is responsible for defining the forward pass of the network. It takes an input tensor x, processes it through the contracting path, bottleneck, and expanding path, and returns the final output.\n",
        "The conv_block method is a helper function for defining a sequence of convolutional and ReLU layers. It takes the number of input channels and output channels and returns a sequential container with the two convolutional layers followed by ReLU activations. This block is used in both the contracting and expanding paths of the U-Net architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoTTJuKNlRHY"
      },
      "outputs": [],
      "source": [
        "# Customize in_channels and out_channels according to your data\n",
        "in_channels = 1  # Number of input channels (1 for single-band images)\n",
        "out_channels = 18  # Number of output channels (number of classes for multi-class segmentation)\n",
        "\n",
        "model = UNet(in_channels=in_channels, out_channels=out_channels)\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruckphe7wae5",
        "outputId": "9643fd2f-84a1-488c-9f6d-15fc129faa3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 2.4161\n",
            "Evaluation loss: 0.0023\n",
            "Epoch 2/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.6596\n",
            "Evaluation loss: 0.0021\n",
            "Epoch 3/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0108\n",
            "Evaluation loss: 0.0002\n",
            "Epoch 4/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0037\n",
            "Evaluation loss: 0.0002\n",
            "Epoch 5/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0011\n",
            "Evaluation loss: 0.0000\n",
            "Epoch 6/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0049\n",
            "Evaluation loss: 0.0012\n",
            "Epoch 7/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0005\n",
            "Evaluation loss: 0.0000\n",
            "Epoch 8/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0001\n",
            "Evaluation loss: 0.0000\n",
            "Epoch 9/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0003\n",
            "Evaluation loss: 0.0000\n",
            "Epoch 10/10\n",
            "----------\n",
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Train Loss: 0.0000\n",
            "Evaluation loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Iterate over each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # Add a counter variable to track the batch number\n",
        "    counter = 0\n",
        "\n",
        "    # Iterate over the training data loader\n",
        "    for batch_idx, (images, masks) in enumerate(train_data_loader):\n",
        "        # Move images and masks to the GPU if available\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Reset the optimizer's gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Perform forward pass with the model\n",
        "        outputs = model(images)\n",
        "        # Debugging suggestion: Check the shapes of outputs and masks\n",
        "        # Print shapes only for the first batch\n",
        "        if counter == 0:\n",
        "          print(f\"Outputs shape: {outputs.shape}\")\n",
        "          print(f\"Masks shape: {masks.shape}\")\n",
        "          counter += 1\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, masks)\n",
        "        # Perform backpropagation\n",
        "        loss.backward()\n",
        "        # Update the model's weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accumulate the training loss\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    # Calculate the average training loss\n",
        "    train_loss /= len(train_data_loader)\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    # Evaluate the model without gradient updates\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the test data loader\n",
        "        for batch_idx, (images, masks) in enumerate(test_data_loader):\n",
        "            # Move images and masks to the GPU if available\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Perform forward pass with the model\n",
        "            outputs = model(images)\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, masks)\n",
        "            \n",
        "            # Accumulate the evaluation loss\n",
        "            eval_loss += loss.item()\n",
        "    \n",
        "    # Calculate the average evaluation loss\n",
        "    eval_loss /= len(test_dataset)\n",
        "    print(f\"Evaluation loss: {eval_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC1LTl2pPpSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ca6dcd-9ca8-498a-ca58-c6bc390a0f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs shape: torch.Size([4, 18, 128, 128])\n",
            "Masks shape: torch.Size([4, 128, 128])\n",
            "Mean IoU score on test dataset: 0.9861\n",
            "Mean precision on test dataset: 1.0000\n",
            "Mean recall on test dataset: 1.0000\n",
            "Mean F1-score on test dataset: 1.0000\n",
            "Mean confusion matrix on test dataset:\n",
            " [[2.42228016e+03 8.17995910e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [6.44171779e-02 6.30633160e+04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "# Read the class mapping from the CSV file\n",
        "class_mapping = read_class_mapping('/content/drive/MyDrive/class_mapping.csv')\n",
        "num_classes = len(class_mapping)\n",
        "\n",
        "# Initialize lists to store metric scores\n",
        "iou_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "mean_confusion_matrices = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Add a counter variable to track the batch number\n",
        "counter = 0\n",
        "\n",
        "# Evaluate the model without gradient updates\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_data_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        # Print shapes only for the first batch\n",
        "        if counter == 0:\n",
        "            print(f\"Outputs shape: {outputs.shape}\")\n",
        "            print(f\"Masks shape: {masks.shape}\")\n",
        "            counter += 1\n",
        "\n",
        "        # Convert masks and predictions to NumPy arrays\n",
        "        masks_np = masks.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        \n",
        "        # Calculate IoU score and append it to the list\n",
        "        iou_score = iou_metric(masks_np, preds_np, class_mapping)\n",
        "        iou_scores.append(iou_score)\n",
        "        \n",
        "        # Calculate precision and append it to the list\n",
        "        precision = precision_score(masks_np.flatten(), preds_np.flatten(), labels=np.arange(num_classes), average='weighted', zero_division=0)\n",
        "        precision_scores.append(precision)\n",
        "        \n",
        "        # Calculate recall and append it to the list\n",
        "        recall = recall_score(masks_np.flatten(), preds_np.flatten(), labels=np.arange(num_classes), average='weighted', zero_division=0)\n",
        "        recall_scores.append(recall)\n",
        "        \n",
        "        # Calculate F1-score and append it to the list\n",
        "        f1 = f1_score(masks_np.flatten(), preds_np.flatten(), labels=np.arange(num_classes), average='weighted', zero_division=0)\n",
        "        f1_scores.append(f1)\n",
        "        \n",
        "        # Calculate the confusion matrix and append it to the list\n",
        "        cm = compute_confusion_matrix(masks_np, preds_np, num_classes)\n",
        "        mean_confusion_matrices.append(cm)\n",
        "\n",
        "# Calculate and print the mean IoU score\n",
        "mean_iou = np.mean(iou_scores)\n",
        "print(f\"Mean IoU score on test dataset: {mean_iou:.4f}\")\n",
        "\n",
        "# Calculate and print the mean precision\n",
        "mean_precision = np.mean(precision_scores)\n",
        "print(f\"Mean precision on test dataset: {mean_precision:.4f}\")\n",
        "\n",
        "# Calculate and print the mean recall\n",
        "mean_recall = np.mean(recall_scores)\n",
        "print(f\"Mean recall on test dataset: {mean_recall:.4f}\")\n",
        "\n",
        "# Calculate and print the mean F1-score\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "print(f\"Mean F1-score on test dataset: {mean_f1:.4f}\")\n",
        "\n",
        "# Calculate and print the mean confusion matrix\n",
        "mean_cm = np.mean(mean_confusion_matrices, axis=0)\n",
        "print(f\"Mean confusion matrix on test dataset:\\n {mean_cm}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gc\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "# Trigger garbage collection\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "DYI_BiKlR-dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e726da7-29f0-40aa-b56b-2fdf23f9a98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6joxEtJLFT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}