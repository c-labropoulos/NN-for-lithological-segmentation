{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-labropoulos/NN-for-lithological-segmentation/blob/main/UNETTEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4C83ubARXLg",
        "outputId": "46cab0e5-4729-4cde-fe33-8c31b56f9f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibfhdxCUZuk9",
        "outputId": "4e136a0e-c4de-4c30-9d24-870f64179503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.22.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2022.12.7)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.3)\n",
            "Collecting click-plugins\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.6 snuggs-1.4.7\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix as sk_confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STK0dobUaAxD",
        "outputId": "5ea035d4-5fb0-4ae3-859a-c022dbcef3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import the PyTorch library\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "# If a GPU with CUDA support is available, the code will use it for computation.\n",
        "# Otherwise, it will use the CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Print the device being used (either \"cuda\" for GPU or \"cpu\" for CPU)\n",
        "# This will help users to verify if their GPU and CUDA installations are recognized\n",
        "# and if the code is running on the GPU as expected.\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUPPQU37BC_N"
      },
      "source": [
        "In the above cell :\n",
        "\n",
        "1.\tWe import the torch library, which is the main PyTorch library for tensor computations and neural network building.\n",
        "2.\tWe check if a GPU with CUDA support is available on the system using torch.cuda.is_available(). If a GPU is available, the code will use it for computation; otherwise, it will use the CPU.\n",
        "3.\tWe create a device variable that holds the device type that will be used for computation (either \"cuda\" for GPU or \"cpu\" for CPU).\n",
        "4.\tWe print the device type being used to help users verify if their GPU and CUDA installations are recognized and if the code is running on the GPU as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G9cSejgPRD7R"
      },
      "outputs": [],
      "source": [
        "def read_raster_image(file_path):\n",
        "    \"\"\"\n",
        "    Read a raster image from the provided file path and return the first band's data as a numpy array.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path of the raster image file to be read.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array containing the data of the first band of the raster image.\n",
        "    \"\"\"\n",
        "    # Use the rasterio library to open the raster image file\n",
        "    with rasterio.open(file_path) as src:\n",
        "        # Read the first band of the image using the `read` method\n",
        "        image_data = src.read(1)\n",
        "\n",
        "    # Return the data of the first band as a numpy array\n",
        "    return image_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsNgvh3UBk-_"
      },
      "source": [
        "In this code snippet:\n",
        "1.\tWe define a read_raster_image function that takes a file path as an argument.\n",
        "2.\tUsing the rasterio.open context manager, we open the raster image file at the provided file path.\n",
        "3.\tWe read the first band of the image using the read method of the rasterio src object and store the data in the image_data variable.\n",
        "4.\tAfter the context manager closes the file, we return the data of the first band as a numpy array.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8OqKGTtdZs67"
      },
      "outputs": [],
      "source": [
        "def get_file_paths(folder_path, file_extension=\".tif\"):\n",
        "    \"\"\"\n",
        "    Traverse a given folder and its subfolders to find files with the specified file extension,\n",
        "    and return a list of their file paths.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path of the folder to search for files.\n",
        "        file_extension (str, optional): The file extension to search for. Defaults to \".tif\".\n",
        "\n",
        "    Returns:\n",
        "        list: A list of file paths for the files with the specified extension found in the folder and its subfolders.\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to store the file paths\n",
        "    file_paths = []\n",
        "\n",
        "    # Use os.walk to traverse the folder and its subfolders\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        # Iterate through the files in the current folder\n",
        "        for file in files:\n",
        "            # Check if the file has the specified extension\n",
        "            if file.endswith(file_extension):\n",
        "                # Append the file path to the list\n",
        "                file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    # Return the list of file paths\n",
        "    return file_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBgdxU8zCc4P"
      },
      "source": [
        "In this code snippet:\n",
        "1.\tWe define a get_file_paths function that takes a folder path and an optional file extension as arguments.\n",
        "2.\tWe initialize an empty list file_paths to store the paths of the files we find.\n",
        "3.\tWe use the os.walk function to traverse the folder and its subfolders, iterating through the files in each folder.\n",
        "4.\tFor each file, we check if it has the specified file extension. If it does, we append its path to the file_paths list.\n",
        "5.\tAfter traversing the entire folder structure, we return the list of file paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8adzu8rVMtI_"
      },
      "outputs": [],
      "source": [
        "# Read class mapping from CSV\n",
        "def read_class_mapping(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    class_mapping = {row['pixel_value']: row['class_name'] for _, row in df.iterrows()}\n",
        "    return class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nzJ4QI-_Mrkf"
      },
      "outputs": [],
      "source": [
        "def iou_metric(y_true, y_pred, class_mapping):\n",
        "    iou_scores = []\n",
        "    \n",
        "    for class_pixel_value in class_mapping:\n",
        "        if class_mapping[class_pixel_value] == 'nan':\n",
        "            continue\n",
        "\n",
        "        y_true_class = y_true == class_pixel_value\n",
        "        y_pred_class = y_pred == class_pixel_value\n",
        "\n",
        "        intersection = np.logical_and(y_true_class, y_pred_class)\n",
        "        union = np.logical_or(y_true_class, y_pred_class)\n",
        "\n",
        "        if np.sum(union) == 0:\n",
        "            continue\n",
        "\n",
        "        iou_score = np.sum(intersection) / np.sum(union)\n",
        "        iou_scores.append(iou_score)\n",
        "\n",
        "    return np.mean(iou_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pixel_accuracy(y_true, y_pred):\n",
        "    correct_pixels = np.sum(y_true == y_pred)\n",
        "    total_pixels = y_true.size\n",
        "    return correct_pixels / total_pixels\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "xw3wwNrdinCW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_metrics(y_true, y_pred, num_classes):\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    \n",
        "    precision = precision_score(y_true_flat, y_pred_flat, labels=np.arange(num_classes), average='weighted')\n",
        "    recall = recall_score(y_true_flat, y_pred_flat, labels=np.arange(num_classes), average='weighted')\n",
        "    f1 = f1_score(y_true_flat, y_pred_flat, labels=np.arange(num_classes), average='weighted')\n",
        "    \n",
        "    return precision, recall, f1\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "s_mjbw8piqKA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_confusion_matrix(y_true, y_pred, num_classes):\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    \n",
        "    cm = sk_confusion_matrix(y_true_flat, y_pred_flat, labels=np.arange(num_classes))\n",
        "    return cm\n"
      ],
      "metadata": {
        "id": "j3aoKDR4ircz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5rLLvVNWYB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C9K5NU0-Tk_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RasterDataset(Dataset):\n",
        "    def __init__(self, root_dir, patch_size, stride, train_ratio, is_train=True):\n",
        "        \"\"\"\n",
        "        Initialize the RasterDataset class.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): The root directory containing the raster images.\n",
        "            patch_size (int): The size of the patches to be extracted from the images.\n",
        "            stride (int): The step size used when extracting patches from the images.\n",
        "            train_ratio (float): The ratio of images to be used for training.\n",
        "            is_train (bool, optional): A flag indicating whether the dataset is for training or testing. Defaults to True.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "        self.train_ratio = train_ratio\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.image_paths = self.get_image_paths(root_dir)\n",
        "\n",
        "        num_train = int(len(self.image_paths) * train_ratio)\n",
        "        if is_train:\n",
        "            self.image_paths = self.image_paths[:num_train]\n",
        "        else:\n",
        "            self.image_paths = self.image_paths[num_train:]\n",
        "\n",
        "        self.image_patches = self.create_patches()\n",
        "\n",
        "    def get_image_paths(self, root_dir):\n",
        "        \"\"\"\n",
        "        Get the sorted file paths for all .tif images in the specified directory.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): The directory containing the raster images.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of sorted file paths for the .tif images in the directory.\n",
        "        \"\"\"\n",
        "        image_folder = os.path.join(root_dir)\n",
        "        image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.tif')])\n",
        "        return image_paths\n",
        "\n",
        "    def create_patches(self):\n",
        "        \"\"\"\n",
        "        Extract patches from the images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of image patches.\n",
        "        \"\"\"\n",
        "        image_patches = []\n",
        "\n",
        "        for image_path in self.image_paths:\n",
        "            image = Image.open(image_path)\n",
        "            width, height = image.size\n",
        "\n",
        "            for y in range(0, height - self.patch_size + 1, self.stride):\n",
        "                for x in range(0, width - self.patch_size + 1, self.stride):\n",
        "                    image_patch = image.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
        "                    image_patches.append(image_patch)\n",
        "\n",
        "        return image_patches\n",
        "\n",
        "    def preprocess(self, image):\n",
        "        \"\"\"\n",
        "        Preprocess an image patch, converting it to a PyTorch tensor and normalizing it.\n",
        "\n",
        "        Args:\n",
        "            image (PIL.Image): The image patch to be preprocessed.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The preprocessed image patch as a PyTorch tensor.\n",
        "        \"\"\"\n",
        "        # Convert the PIL Image to a numpy array\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Normalize the image to the range [0, 1]\n",
        "        image = image / 255.0\n",
        "\n",
        "        # Convert the numpy array to a PyTorch tensor and add a channel dimension\n",
        "        image = torch.from_numpy(image).float().unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the number of image patches in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of image patches.\n",
        "        \"\"\"\n",
        "        return len(self.image_patches)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Get an image patch and its corresponding mask from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): The index of the image patch in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the image patch and its corresponding mask as PyTorch tensors.\n",
        "        \"\"\"\n",
        "        image_patch = self.image_patches[index]\n",
        "        image_patch = self.preprocess(image_patch)\n",
        "        mask = (image_patch != 0).long() # Convert the mask tensor to Long\n",
        "        mask = mask.squeeze(0) # Remove the channel dimension\n",
        "        return image_patch, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y8SiDPA5bAjC"
      },
      "outputs": [],
      "source": [
        "# Create dataset and data loader\n",
        "folder_path = \"/content/drive/MyDrive/raster_to_be_used\"\n",
        "\n",
        "# Set the parameters for creating the dataset\n",
        "patch_size = 128  # Size of the patches extracted from the images\n",
        "stride = 32  # Stride used when extracting patches from the images\n",
        "train_ratio = 0.8  # Ratio of the dataset to be used for training\n",
        "\n",
        "# Create the training dataset using the RasterDataset class\n",
        "train_dataset = RasterDataset(folder_path, patch_size, stride, train_ratio, is_train=True)\n",
        "# Create the DataLoader for the training dataset, with a batch size of 4, shuffling the data, and using 2 workers for parallel processing\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Create the testing dataset using the RasterDataset class\n",
        "test_dataset = RasterDataset(folder_path, patch_size, stride, train_ratio, is_train=False)\n",
        "# Create the DataLoader for the testing dataset, with a batch size of 4, without shuffling the data, and using 2 workers for parallel processing\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vl21FWF4ZA"
      },
      "source": [
        "In the above code snippet, we create the datasets and data loaders for both training and testing. The parameters for patch size, stride, and train ratio are set. The DataLoader objects are created with specific batch sizes, shuffling options, and numbers of parallel processing workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "waMf4K-vX71m"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Expanding path\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = self.conv_block(1024, 512)\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = self.conv_block(512, 256)\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = self.conv_block(256, 128)\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        upconv4 = self.upconv4(bottleneck)\n",
        "        dec4 = self.dec4(torch.cat((enc4, upconv4), dim=1))\n",
        "        upconv3 = self.upconv3(dec4)\n",
        "        dec3 = self.dec3(torch.cat((enc3, upconv3), dim=1))\n",
        "        upconv2 = self.upconv2(dec3)\n",
        "        dec2 = self.dec2(torch.cat((enc2, upconv2), dim=1))\n",
        "        upconv1 = self.upconv1(dec2)\n",
        "        dec1 = self.dec1(torch.cat((enc1, upconv1), dim=1))\n",
        "\n",
        "        return self.final_conv(dec1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctbld2QuIUab"
      },
      "source": [
        "In the code snippet above, a UNet class is defined, which inherits from PyTorch's nn.Module. The U-Net architecture consists of an encoder (contracting path), a bottleneck, and a decoder (expanding path). The U-Net is designed for semantic segmentation tasks.\n",
        "The __init__ method of the UNet class defines the layers in the contracting path, the bottleneck, and the expanding path.\n",
        "The forward method is responsible for defining the forward pass of the network. It takes an input tensor x, processes it through the contracting path, bottleneck, and expanding path, and returns the final output.\n",
        "The conv_block method is a helper function for defining a sequence of convolutional and ReLU layers. It takes the number of input channels and output channels and returns a sequential container with the two convolutional layers followed by ReLU activations. This block is used in both the contracting and expanding paths of the U-Net architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uoTTJuKNlRHY"
      },
      "outputs": [],
      "source": [
        "# Customize in_channels and out_channels according to your data\n",
        "in_channels = 1  # Number of input channels (1 for single-band images)\n",
        "out_channels = 18  # Number of output channels (number of classes for multi-class segmentation)\n",
        "\n",
        "model = UNet(in_channels=in_channels, out_channels=out_channels)\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruckphe7wae5",
        "outputId": "3ffd7c41-8787-4961-8396-8279896776e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 603.3939\n",
            "Evaluation loss: 0.0055\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.0103\n",
            "Evaluation loss: 0.0006\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.0068\n",
            "Evaluation loss: 0.0004\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.0052\n",
            "Evaluation loss: 0.0003\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.0044\n",
            "Evaluation loss: 0.0002\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.0040\n",
            "Evaluation loss: 0.0001\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 810.6022\n",
            "Evaluation loss: 0.0013\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0068\n",
            "Evaluation loss: 0.0009\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0053\n",
            "Evaluation loss: 0.0005\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0034\n",
            "Evaluation loss: 0.0003\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "# Iterate over each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    # Iterate over the training data loader\n",
        "    for batch_idx, (images, masks) in enumerate(train_data_loader):\n",
        "        # Move images and masks to the GPU if available\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Reset the optimizer's gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Perform forward pass with the model\n",
        "        outputs = model(images)\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, masks)\n",
        "        # Perform backpropagation\n",
        "        loss.backward()\n",
        "        # Update the model's weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accumulate the training loss\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    # Calculate the average training loss\n",
        "    train_loss /= len(train_data_loader)\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    # Evaluate the model without gradient updates\n",
        "    with torch.no_grad():\n",
        "        # Iterate over the test data loader\n",
        "        for batch_idx, (images, masks) in enumerate(test_data_loader):\n",
        "            # Move images and masks to the GPU if available\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Perform forward pass with the model\n",
        "            outputs = model(images)\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, masks)\n",
        "            \n",
        "            # Accumulate the evaluation loss\n",
        "            eval_loss += loss.item()\n",
        "    \n",
        "    # Calculate the average evaluation loss\n",
        "    eval_loss /= len(test_dataset)\n",
        "    print(f\"Evaluation loss: {eval_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WC1LTl2pPpSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5cf901-c048-4c4e-d4b6-41b2d1d468a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean IoU score on test dataset: 0.8920\n",
            "Mean precision on test dataset: 0.9998\n",
            "Mean recall on test dataset: 0.9998\n",
            "Mean F1-score on test dataset: 0.9998\n",
            "Mean confusion matrix on test dataset:\n",
            " [[2.40848262e+03 1.38793456e+01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.22392638e+00 6.30621564e+04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class_mapping = read_class_mapping('/content/drive/MyDrive/class_mapping.csv')\n",
        "num_classes = len(class_mapping)\n",
        "\n",
        "iou_scores = []\n",
        "precision_scores = []  # Add this line\n",
        "recall_scores = []     # Add this line\n",
        "f1_scores = []         # Add this line\n",
        "mean_confusion_matrices = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_data_loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        \n",
        "        masks_np = masks.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "        \n",
        "        iou_score = iou_metric(masks_np, preds_np, class_mapping)\n",
        "        iou_scores.append(iou_score)\n",
        "        \n",
        "        precision = precision_score(masks_np.flatten(), preds_np.flatten(), labels=np.arange(num_classes), average='weighted', zero_division=0)\n",
        "        precision_scores.append(precision)\n",
        "        \n",
        "        recall = recall_score(masks_np.flatten(), preds_np.flatten(), labels=np.arange(num_classes), average='weighted', zero_division=0)\n",
        "        recall_scores.append(recall)\n",
        "        \n",
        "        f1 = f1_score(masks_np.flatten(), preds_np.flatten(), labels=np.arange(num_classes), average='weighted', zero_division=0)\n",
        "        f1_scores.append(f1)\n",
        "        \n",
        "        cm = compute_confusion_matrix(masks_np, preds_np, num_classes)\n",
        "        mean_confusion_matrices.append(cm)\n",
        "\n",
        "mean_iou = np.mean(iou_scores)\n",
        "print(f\"Mean IoU score on test dataset: {mean_iou:.4f}\")\n",
        "\n",
        "mean_precision = np.mean(precision_scores)\n",
        "print(f\"Mean precision on test dataset: {mean_precision:.4f}\")\n",
        "\n",
        "mean_recall = np.mean(recall_scores)\n",
        "print(f\"Mean recall on test dataset: {mean_recall:.4f}\")\n",
        "\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "print(f\"Mean F1-score on test dataset: {mean_f1:.4f}\")\n",
        "\n",
        "mean_cm = np.mean(mean_confusion_matrices, axis=0)\n",
        "print(f\"Mean confusion matrix on test dataset:\\n {mean_cm}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gc\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "# Trigger garbage collection\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "DYI_BiKlR-dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f278e5c9-9da2-4168-a23c-c3ef8e9a9b95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6joxEtJLFT"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}