{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1o6bX9ZtgdC5rxltKzvqhHtBtzP74tS5m",
      "authorship_tag": "ABX9TyPIkIf5alUFSoPwqOppZGzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-labropoulos/NN-for-lithological-segmentation/blob/main/UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "H4C83ubARXLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c930375-2fac-43a5-d81e-72cb9168b854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibfhdxCUZuk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8114aa-8f67-4410-f2ad-c4403ed3ff90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.9/dist-packages (1.3.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.9/dist-packages (from rasterio) (8.1.3)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.9/dist-packages (from rasterio) (1.22.4)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.9/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.9/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.9/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from rasterio) (2022.12.7)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.9/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio\n",
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Verify GPU and CUDA installations\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "STK0dobUaAxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d9ea04-fe4f-4a7e-a6c5-c36c26469f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_raster_image(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        image_data = src.read(1)  # Read the first band\n",
        "    return image_data\n"
      ],
      "metadata": {
        "id": "G9cSejgPRD7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_paths(folder_path, file_extension=\".tif\"):\n",
        "    file_paths = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(file_extension):\n",
        "                file_paths.append(os.path.join(root, file))\n",
        "    return file_paths\n"
      ],
      "metadata": {
        "id": "8OqKGTtdZs67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class RasterDataset(Dataset):\n",
        "    def __init__(self, root_dir, patch_size, stride, train_ratio, is_train=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "        self.train_ratio = train_ratio\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.image_paths = self.get_image_paths(root_dir)\n",
        "\n",
        "        num_train = int(len(self.image_paths) * train_ratio)\n",
        "        if is_train:\n",
        "            self.image_paths = self.image_paths[:num_train]\n",
        "        else:\n",
        "            self.image_paths = self.image_paths[num_train:]\n",
        "\n",
        "        self.image_patches = self.create_patches()\n",
        "\n",
        "    def get_image_paths(self, root_dir):\n",
        "        image_folder = os.path.join(root_dir)\n",
        "        image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.tif')])\n",
        "        return image_paths\n",
        "\n",
        "    def create_patches(self):\n",
        "        image_patches = []\n",
        "\n",
        "        for image_path in self.image_paths:\n",
        "            image = Image.open(image_path)\n",
        "            width, height = image.size\n",
        "\n",
        "            for y in range(0, height - self.patch_size + 1, self.stride):\n",
        "                for x in range(0, width - self.patch_size + 1, self.stride):\n",
        "                    image_patch = image.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
        "                    image_patches.append(image_patch)\n",
        "\n",
        "        return image_patches\n",
        "\n",
        "    def preprocess(self, image):\n",
        "        # Convert the PIL Image to a numpy array\n",
        "        image = np.array(image)\n",
        "\n",
        "        # Normalize the image to the range [0, 1]\n",
        "        image = image / 255.0\n",
        "\n",
        "        # Convert the numpy array to a PyTorch tensor and add a channel dimension\n",
        "        image = torch.from_numpy(image).float().unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_patches)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "         image_patch = self.image_patches[index]\n",
        "         image_patch = self.preprocess(image_patch)\n",
        "         mask = (image_patch != 0).long()  # Convert the mask tensor to Long\n",
        "         mask = mask.squeeze(0)  # Remove the channel dimension\n",
        "         return image_patch, mask"
      ],
      "metadata": {
        "id": "C9K5NU0-Tk_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and data loader\n",
        "folder_path = \"/content/drive/MyDrive/raster_to_be_used\"\n",
        "\n",
        "patch_size = 128\n",
        "stride = 32\n",
        "train_ratio = 0.8\n",
        "\n",
        "train_dataset = RasterDataset(folder_path, patch_size, stride, train_ratio, is_train=True)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = RasterDataset(folder_path, patch_size, stride, train_ratio, is_train=False)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "Y8SiDPA5bAjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Expanding path\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = self.conv_block(1024, 512)\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = self.conv_block(512, 256)\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = self.conv_block(256, 128)\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = self.conv_block(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        upconv4 = self.upconv4(bottleneck)\n",
        "        dec4 = self.dec4(torch.cat((enc4, upconv4), dim=1))\n",
        "        upconv3 = self.upconv3(dec4)\n",
        "        dec3 = self.dec3(torch.cat((enc3, upconv3), dim=1))\n",
        "        upconv2 = self.upconv2(dec3)\n",
        "        dec2 = self.dec2(torch.cat((enc2, upconv2), dim=1))\n",
        "        upconv1 = self.upconv1(dec2)\n",
        "        dec1 = self.dec1(torch.cat((enc1, upconv1), dim=1))\n",
        "\n",
        "        return self.final_conv(dec1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "waMf4K-vX71m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Customize in_channels and out_channels according to your data\n",
        "in_channels = 1  # Number of input channels (1 for single-band images)\n",
        "out_channels = 18  # Number of output channels (number of classes for multi-class segmentation)\n",
        "\n",
        "model = UNet(in_channels=in_channels, out_channels=out_channels)\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "uoTTJuKNlRHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for batch_idx, (images, masks) in enumerate(train_data_loader):  # Use train_data_loader\n",
        "        # Move images and masks to the GPU\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    train_loss /= len(train_data_loader)\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, masks) in enumerate(test_data_loader):  # Use test_data_loader\n",
        "            # Move images and masks to the GPU\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "    \n",
        "    eval_loss /= len(test_dataset)  # Replace len(dataset, is_train=False) with len(test_dataset)\n",
        "    print(f\"Evaluation loss: {eval_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Ruckphe7wae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed02648-d3da-49da-c02e-dccdcdc0ecab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 13.9123\n",
            "Evaluation loss: 0.0038\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.0027\n",
            "Evaluation loss: 0.0011\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.0005\n",
            "Evaluation loss: 0.0001\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.0003\n",
            "Evaluation loss: 0.0003\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.0145\n",
            "Evaluation loss: 0.0033\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.0018\n",
            "Evaluation loss: 0.0006\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.0023\n",
            "Evaluation loss: 0.0041\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0003\n",
            "Evaluation loss: 0.0001\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0002\n",
            "Evaluation loss: 0.0000\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0001\n",
            "Evaluation loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NA6joxEtJLFT"
      }
    }
  ]
}